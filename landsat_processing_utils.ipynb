{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46c2776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Attempt to create a TOA_reflectance_stacker with the appropriate arguments and flags\n",
    "import argparse\n",
    "from glob import glob\n",
    "import os, sys\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import earthpy.spatial as es\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f9bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Additional utils\n",
    "\n",
    "def histogram_stretch(img, min_vals = None, max_vals = 99):\n",
    "    \"\"\"\n",
    "    Performs a histogram_stretch on an image. DO NOT use this for analytical workflows - \n",
    "    this should only be used to improve image visualization\n",
    "    \n",
    "    img: an unmasked 3D raster \n",
    "    \n",
    "    min_vals: percentile that you wish to crop to\n",
    "        will be np.zeros by default\n",
    "    max_vals: percentile that you wish to crop to\n",
    "        will be np.nanpercentile(img, 99) by default # crops to 99th percentile\n",
    "    \"\"\"\n",
    "    if img.ndim != 3:\n",
    "        print(\"ValueError: Your raster must have three dimensions.\")\n",
    "        return\n",
    "    \n",
    "    # This returns the max_valth percentile\n",
    "    max_vals = np.nanpercentile(img, max_vals, axis = (1,2)).reshape(img.shape[0],1,1) \n",
    "    # min_vals = np.nanmin(tcc_toa, axis = (1,2)).reshape(3,1,1) # Use this to stretch to minimum values\n",
    "    if min_vals is not None:\n",
    "        min_vals = np.nanpercentile(img, min_vals, axis = (1,2)).reshape(img.shape[0],1,1)\n",
    "    else:\n",
    "        min_vals = np.zeros(img.shape[0]).reshape(img.shape[0],1,1)\n",
    "    \n",
    "    # Perform normalization\n",
    "    img_stretched = (img - min_vals) / (max_vals - min_vals)\n",
    "    \n",
    "    # Clip values above 1\n",
    "    img_stretched[img_stretched > 1] = 1\n",
    "    \n",
    "    return img_stretched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ece78fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TOA Processing Utils\n",
    "\n",
    "def parse_mtl(mtl_file):\n",
    "    \"\"\"\n",
    "    Parses the landsat metadata file into a dictionary of dictionaries.\n",
    "    \n",
    "    Dictionary is split into several sub-dicts including PRODUCT_CONTENTS and IMAGE_ATTRIBUTES\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(mtl_file) as f:\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "\n",
    "    clean_lines = [element.strip(\"\\n\").strip() for element in lines]\n",
    "\n",
    "    ### PARSE THE MTL FILE INTO A DICTIONARY ###\n",
    "    # Find all major groups in the metadata\n",
    "    groups = [element for element in clean_lines if element.startswith(\"GROUP\")]\n",
    "\n",
    "    group_dict = dict()\n",
    "\n",
    "    # We don't need the overarching metadata group\n",
    "    for group in groups[1:]:\n",
    "        # Return the part of list that the group contains\n",
    "        contents = clean_lines[clean_lines.index(group)+1:clean_lines.index(f\"END_{group}\")]\n",
    "\n",
    "        data_dict = {}\n",
    "        # Iterate through the elements in the list\n",
    "        for element in contents:\n",
    "            # Split the element by \"=\"\n",
    "            parts = element.split(\"=\")\n",
    "            if len(parts) == 2:\n",
    "                # Assign A as key and B as value to the dictionary\n",
    "                key = parts[0].strip()  # Remove leading/trailing whitespace\n",
    "                value = parts[1].strip()  # Remove leading/trailing whitespace\n",
    "                data_dict[key] = value.strip(\"\\\"\") # Remove quotation marks\n",
    "\n",
    "        group_dict[group.replace(\"GROUP = \", \"\", 1)] = data_dict\n",
    "    \n",
    "    return group_dict\n",
    "\n",
    "def toa_reflectance(raster, band_num, metadata, sun_corr = True):\n",
    "    \"\"\"\n",
    "    raster: requires a 2D numpy array as read from rasterio\n",
    "    NB - array should be masked since landsat uses 0 for np.nan\n",
    "    \n",
    "    band_num: the landsat band number associated with that raster\n",
    "    sun_corr: indicate if you want sun elevation correction (default true)\n",
    "    \n",
    "    returns the landsat level 1 product raster corrected for TOA\n",
    "    Note that these are center image sun corrected - you can do pixel level sun correction but it\n",
    "    takes a lot more work\n",
    "    \"\"\"\n",
    "    # Get TOA reflectance\n",
    "    toa_ref = raster * float(metadata[\"LEVEL1_RADIOMETRIC_RESCALING\"][f\"REFLECTANCE_MULT_BAND_{band_num}\"]) + float(metadata[\"LEVEL1_RADIOMETRIC_RESCALING\"][f\"REFLECTANCE_ADD_BAND_{band_num}\"])\n",
    "    \n",
    "    # Correct for sun elevation\n",
    "    if sun_corr:\n",
    "        toa_ref = toa_ref / np.sin(np.deg2rad(float(metadata[\"IMAGE_ATTRIBUTES\"][\"SUN_ELEVATION\"])))\n",
    "    \n",
    "    # Clip any values that are larger than 1 to 1\n",
    "    toa_ref[toa_ref > 1] = 1\n",
    "    \n",
    "    return toa_ref\n",
    "\n",
    "def resample_tif(raster_file, target_height, target_width):\n",
    "    \"\"\"\n",
    "    given a raster file and a height/width with the same aspect ratio, \n",
    "    \n",
    "    output a masked 2D array of resampled data\n",
    "    \"\"\"\n",
    "    # we need to resample the land_use geotiff because it has a 10m scale\n",
    "    with rio.open(raster_file) as dataset:\n",
    "\n",
    "        # resample data to target shape\n",
    "        data = dataset.read(\n",
    "            out_shape=(\n",
    "                dataset.count,\n",
    "                target_height, # height and width of the up/downsampled tiff - this will force the \n",
    "                target_width  # opened landuse dataset into this shape\n",
    "            ),\n",
    "            resampling=Resampling.nearest, # nearest is good for land use, use cubicspline for DEM\n",
    "            masked = True\n",
    "        )\n",
    "\n",
    "        # scale image transform object\n",
    "        transform = dataset.transform * dataset.transform.scale(\n",
    "            (dataset.width / data.shape[-1]),\n",
    "            (dataset.height / data.shape[-2])\n",
    "        )\n",
    "\n",
    "        resampled_profile = dataset.profile\n",
    "        resampled_profile.update(transform = transform, \n",
    "                       width = data.shape[-1], \n",
    "                       height = data.shape[-2])\n",
    "\n",
    "    dataset.close()\n",
    "    \n",
    "    # data = masked numpy array, squeeze removes any dimensions of length 1 (i.e., 3D array with only\n",
    "    # one stack will be converted into a 2D array)\n",
    "    return data.squeeze(), resampled_profile\n",
    "\n",
    "def earthpy_cropper(filenames, target_shapefile, profile, cleanup = False):\n",
    "    '''\n",
    "    Crops files to the bounds of the specified shapefile\n",
    "    \n",
    "    filenames: list of landsat images\n",
    "    target_shapefile: this should be a shapely polygon. you can get this by using .total_bounds\n",
    "    profile: rasterio metadata object - used to convert the shapefile to the raster CRS\n",
    "    cleanup: indicates whether the original raster files should be deleted. This will save a lot of disk space.\n",
    "    \n",
    "    returns a list of band_paths \n",
    "    '''\n",
    "    \n",
    "    target_shapefile = gpd.read_file(target_shapefile)\n",
    "    \n",
    "    # convert to the correct CRS\n",
    "    target_shapefile = target_shapefile.to_crs(profile['crs'].to_epsg())\n",
    "    xmin, ymin, xmax, ymax = target_shapefile.total_bounds\n",
    "    target_shapefile = box(np.floor(xmin), np.floor(ymin), np.ceil(xmax), np.ceil(ymax))\n",
    "    \n",
    "    band_paths = es.crop_all(\n",
    "        filenames, os.path.dirname(filenames[0]), [target_shapefile], overwrite=True\n",
    "    )\n",
    "\n",
    "    # do this to delete original files to save space\n",
    "    # only uncomment if you are SURE YOU WANT TO DO THIS\n",
    "    if cleanup:\n",
    "        for file in filenames:\n",
    "            if \"TOA\" in file:\n",
    "                os.remove(file)\n",
    "            \n",
    "    return band_paths\n",
    "\n",
    "def process_folder(filepath, mask = None, stack = False, sun_corr = True, outdir = None, cleanup = False):\n",
    "    '''\n",
    "    Path to the landsat folder containing the landsat images as well as the MTL.txt file.    \n",
    "    '''\n",
    "    \n",
    "    # identify the mtl_file\n",
    "    mtl_file = glob(os.path.join(filepath, '*MTL.txt'))\n",
    "    \n",
    "    # If no mtl_file found or more than one mtl_file found\n",
    "    if len(mtl_file) != 1:\n",
    "        print('No MTL file found or more than one MTL file found. Please check the folder.')\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        metadata = parse_mtl(mtl_file[0])\n",
    "    except:\n",
    "        print(\"Metadata could not be read.\")\n",
    "    \n",
    "    # Find all bands\n",
    "    filenames = [value for key, value in metadata[\"PRODUCT_CONTENTS\"].items() if key.startswith(\"FILE_NAME_BAND\")]\n",
    "    \n",
    "    # convert to the correct filenames\n",
    "    filenames = [os.path.join(filepath, filename) for filename in filenames]\n",
    "    \n",
    "    # Create an initial profile from band 1. This will be used for transformations and defaults.\n",
    "    with rio.open(filenames[0]) as src:\n",
    "        profile = src.profile\n",
    "        src.close()\n",
    "        \n",
    "    profile.update(dtype = np.float32, nodata = np.nan)\n",
    "    \n",
    "    processed_filenames = list()\n",
    "    \n",
    "    for band in tqdm(filenames):\n",
    "        \n",
    "        # get band number\n",
    "        band_num = int(band.split(\"B\")[1].split(\".\")[0])\n",
    "        \n",
    "        # Skip band 8 completely (panchromatic band)\n",
    "        if band_num == 8:\n",
    "            continue\n",
    "        \n",
    "        # if no output directory specified, place the TOA corrected rasters in the original folder\n",
    "        if outdir is not None:\n",
    "            out_path = os.path.join(outdir, f'{os.path.basename(band).split(\".\")[0]}_TOA.tif')\n",
    "        else:\n",
    "            out_path = os.path.join(filepath, f'{os.path.basename(band).split(\".\")[0]}_TOA.tif')\n",
    "        \n",
    "        # catch temperature bands\n",
    "        if band_num > 9:\n",
    "            processed_filenames.append(band)\n",
    "            continue\n",
    "        \n",
    "        # read original raster\n",
    "        with rio.open(band) as src:\n",
    "            raster = src.read(1, masked = True)\n",
    "            src.close()\n",
    "        \n",
    "        # perform correction\n",
    "        toa_ref = toa_reflectance(raster, band_num, metadata, sun_corr = sun_corr)\n",
    "        \n",
    "        # save corrected image (note change to dtype float32 to accommodate np.nan)\n",
    "        with rio.open(out_path, 'w', **profile) as dst:\n",
    "            dst.write(toa_ref.astype(np.float32), 1)\n",
    "            dst.close()\n",
    "        \n",
    "        # add filepaths to processed_filenames\n",
    "        processed_filenames.append(out_path)\n",
    "        \n",
    "    # Crop the data if a shapefile has been provided\n",
    "    # NB this always produces a rectangular raster based on the total_bounds_ of the given polygon\n",
    "    \n",
    "    if mask is not None:\n",
    "        processed_filenames = earthpy_cropper(processed_filenames, mask, profile, cleanup)\n",
    "    \n",
    "    # If stack, create stacked data\n",
    "    # NB: THIS WILL BUG OUT IF TEMP DATA IS NOT SAME RESOLUTION AS OTHER BAND DATA\n",
    "    if stack:\n",
    "        \n",
    "        if outdir is not None:\n",
    "            out_path = os.path.join(outdir, f\"{os.path.basename(filepath)}_TOA_STACKED.tif\")\n",
    "        else:\n",
    "            out_path = os.path.join(filepath, f\"{os.path.basename(filepath)}_TOA_STACKED.tif\")\n",
    "        \n",
    "        try:\n",
    "            stack, metadata = es.stack(processed_filenames, out_path = out_path)\n",
    "        except:\n",
    "            print('Issue with stacking - attempting to stack bands 1-7 only.')\n",
    "            try:\n",
    "                stack, metadata = es.stack(processed_filenames[:7], out_path = out_path)\n",
    "            except Exception as e:\n",
    "                print('Issue with stacking - please see error below for more info')\n",
    "                print(e)\n",
    "                \n",
    "    print('TOA calculation complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f78dc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-m MASK] [-s] [-c] [-o OUTDIR] [-d] filepath\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ### useful tutorial https://docs.python.org/3/howto/argparse.html#argparse-tutorial\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Calculates TOA reflectance for Level 1 Landsat products. Additional flags can be set to crop, \\\n",
    "        stack, and ignore sun elevation correction.')\n",
    "\n",
    "    # add positional argument (i.e., required argument)\n",
    "    parser.add_argument('filepath')\n",
    "\n",
    "    # optional flags - the name of the variable is the -- option\n",
    "    parser.add_argument('-m', '--mask', help = 'Provide a shapefile that the images will be cropped to') \n",
    "\n",
    "    # on/off flags - action indicates what the program should do\n",
    "    # if flag is called (default will be the opposite for on/off)\n",
    "    parser.add_argument('-s', '--stack', action='store_true', help = 'Create a stacked raster of the images - default False') \n",
    "    parser.add_argument('-c', '--sun_corr', action='store_false', help = \"add this flag if you DON'T want to do a sun elevation correction - default True\")\n",
    "    parser.add_argument('-o', '--outdir', help = \"Specify an output folder to save TOA corrected images\")\n",
    "    parser.add_argument('-d', '--cleanup', action='store_true', help = \"If cropping, choose whether to delete the uncropped TOA images - default False\")\n",
    "\n",
    "\n",
    "    ### Preview arguments\n",
    "    # parser.parse_args('LC08 -c -m test.geojson'.split(' '))\n",
    "    \n",
    "    # grab arguments from command line\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # calculate TOA\n",
    "    process_folder(args.filepath, \n",
    "                   mask = args.mask, \n",
    "                   stack = args.stack, \n",
    "                   sun_corr = args.sun_corr, \n",
    "                   outdir = args.outdir, \n",
    "                   cleanup = args.cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8fe510dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(filepath='\"LC08_L1TP_180035_20230710_20230718_02_T1\"', mask='\"study_area.geojson\"', stack=True, sun_corr=False, outdir='\"20230710_TOA\"', cleanup=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse_args('-m \"study_area.geojson\" -s -c -o \"20230710_TOA\" \"LC08_L1TP_180035_20230710_20230718_02_T1\"'.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb8a59b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LC08_L1TP_180035_20230710_20230718_02_T1'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd417d78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gdal_new]",
   "language": "python",
   "name": "conda-env-gdal_new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
